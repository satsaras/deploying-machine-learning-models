{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Amazon Reviews Classification using Pyspark libraries.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPhViCVeONG1l9hOjdoKBo2",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/satsaras/deploying-machine-learning-models/blob/master/Amazon_Reviews_Classification_using_Pyspark_libraries.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsTolzvRiirc"
      },
      "source": [
        "# This notebook utilises pyspark libraries to perform text classification on Amazon reviews corpus. The data has 3.6M reviews in training data set with equal distribution for Negative and positive class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bic26tQka5Xw"
      },
      "source": [
        "Experiments tried:\n",
        "\n",
        "\n",
        "1.   TFIDF based features trained with Logistic and Random Forest\n",
        "2.   Doc2vec based features with Logistic Regression\n",
        "3. Doc2vec Features trained on DeepLearning model with Keras+Elephas(extension of Keras to run model using spark)\n",
        "4. Doc2vec based features trained on >1M data with Logistic Regression\n",
        "5. Bert based sentence embeddings from SparkNLP trained on Deep Learning Model\n",
        "6. Universal Sentence Encoder based sentence embeddings from Spark NLP trained on Deep Learning Model\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FucbGD5FbCyJ",
        "outputId": "046eb7c6-a64f-43c4-e7ec-76867a47c4e9"
      },
      "source": [
        "!pip install kaggle"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.41.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.5.30)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iWryAfo01t-"
      },
      "source": [
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "id": "wq2b6t0f1Hai",
        "outputId": "613334fd-8a18-4d2c-b87a-a929a72b9b0d"
      },
      "source": [
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-425dfd22-ef2c-4e1a-8c3a-2a05d4b10d85\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-425dfd22-ef2c-4e1a-8c3a-2a05d4b10d85\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5tS5JN71bPV"
      },
      "source": [
        "!rm -r ~/.kaggle\n",
        "!mkdir ~/.kaggle\n",
        "!mv ./kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcaZPCl5srGZ"
      },
      "source": [
        "Read Data from kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWz5SVNS2q6W",
        "outputId": "b031bf34-3a4f-483d-b954-40ee63580871"
      },
      "source": [
        "!kaggle datasets download -d kritanjalijain/amazon-reviews"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading amazon-reviews.zip to /content\n",
            " 99% 1.29G/1.29G [00:17<00:00, 115MB/s]\n",
            "100% 1.29G/1.29G [00:17<00:00, 80.8MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71taB2f_3iBi",
        "outputId": "0a7bf7ce-37d5-4c57-c0e6-70c28fe11f79"
      },
      "source": [
        "! unzip /content/amazon-reviews.zip -d /content/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/amazon-reviews.zip\n",
            "  inflating: /content/amazon_review_polarity_csv.tgz  \n",
            "  inflating: /content/test.csv       \n",
            "  inflating: /content/train.csv      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SX5hsh5K4GgS"
      },
      "source": [
        "!pip install pyspark\n",
        "!pip install elephas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pblwQFScq5Lk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffd759c2-abdc-4324-fb2d-44e55b3d7c2e"
      },
      "source": [
        "!pip install h5py==2.10.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: h5py==2.10.0 in /usr/local/lib/python3.7/dist-packages (2.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oi0GIHj5qaXL"
      },
      "source": [
        "!pip install tensorflow==2.5.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T99hctU04Kj9"
      },
      "source": [
        "import pyspark\n",
        "from pyspark import SparkContext,SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer,CountVectorizer\n",
        "from pyspark.ml.feature import Word2Vec\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import LogisticRegression,RandomForestClassifier, GBTClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.feature import HashingTF, Tokenizer\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "import pandas as pd\n",
        "from pyspark.ml.feature import Word2Vec\n",
        "from pyspark.sql.functions import when,concat_ws,col,rand\n",
        "from pyspark.sql.types import *\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers, regularizers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import optimizers\n",
        "\n",
        "from elephas.ml_model import ElephasEstimator \n",
        "from elephas.spark_model import SparkModel\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YElzKxSvQ5JN"
      },
      "source": [
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSDSQ9qHsy47"
      },
      "source": [
        "Initiate Sparksession with some spark configs to increase defaults like increasing default memory used per executor from 1 gb to 6gb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_b4K4qD4U8B"
      },
      "source": [
        "spark=SparkSession.builder\\\n",
        ".config('spark.driver.memory',\"6g\")\\\n",
        ".config('spar.executor.cores','6')\\\n",
        ".config('spark.executor.memory','6g')\\\n",
        ".config('spark.master','local[*]')\\\n",
        ".appName('test')\\\n",
        ".getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HoI1TH-s2eN"
      },
      "source": [
        "Read data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFNC4ECK4XFd"
      },
      "source": [
        "df=spark.read.csv('/content/train.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faRHxAeiFagB",
        "outputId": "3d8cb107-46a0-482c-fd37-753c4592fae5"
      },
      "source": [
        "df.explain()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "== Physical Plan ==\n",
            "FileScan csv [_c0#16,_c1#17,_c2#18] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/content/train.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<_c0:string,_c1:string,_c2:string>\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDaQI52Rs46z"
      },
      "source": [
        "Reanme Columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdEvPTh-4bmt"
      },
      "source": [
        "df=df.withColumnRenamed('_c0','Polarity')\n",
        "df=df.withColumnRenamed('_c1','Title')\n",
        "df=df.withColumnRenamed('_c2','Review')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XWZ4eaBtugU"
      },
      "source": [
        "Filter out small reviews as they will not provide enough signal, will not be using Titel of review as it may have high polarity words like 'Amazing', 'Worst' etc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpefnAX84iDQ"
      },
      "source": [
        "df=df.withColumn(\"len_Review\", F.length(\"Review\"))\n",
        "df=df.filter(df['len_Review']>10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTz4ldY2sc-_"
      },
      "source": [
        "Add custom stop words(that are high indicator of polarity) to increase generalization capability of model, model should be able to predict based on entire context of text rather than just few obvious words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MR6QWp_tzhj"
      },
      "source": [
        "custom_stopwords=frozenset(['good','great','bad','hate','like','better','recommend','best','disappointed','perfect','return','easy','hard','ha','wa','love','problem','worst','boring','wrong','loved','wonderful','amazing'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akJyqH5Z9n0P"
      },
      "source": [
        "generic_stopwords=frozenset(['all', 'six', 'just', 'less', 'being', 'indeed', 'over', 'move', 'anyway', 'four', 'not', 'own', 'through',\n",
        "    'using', 'fifty', 'where', 'mill', 'only', 'find', 'before', 'one', 'whose', 'system', 'how', 'somewhere',\n",
        "    'much', 'thick', 'show', 'had', 'enough', 'should', 'to', 'must', 'whom', 'seeming', 'yourselves', 'under',\n",
        "    'ours', 'two', 'has', 'might', 'thereafter', 'latterly', 'do', 'them', 'his', 'around', 'than', 'get', 'very',\n",
        "    'de', 'none', 'cannot', 'every', 'un', 'they', 'front', 'during', 'thus', 'now', 'him', 'nor', 'name', 'regarding',\n",
        "    'several', 'hereafter', 'did', 'always', 'who', 'didn', 'whither', 'this', 'someone', 'either', 'each', 'become',\n",
        "    'thereupon', 'sometime', 'side', 'towards', 'therein', 'twelve', 'because', 'often', 'ten', 'our', 'doing', 'km',\n",
        "    'eg', 'some', 'back', 'used', 'up', 'go', 'namely', 'computer', 'are', 'further', 'beyond', 'ourselves', 'yet',\n",
        "    'out', 'even', 'will', 'what', 'still', 'for', 'bottom', 'mine', 'since', 'please', 'forty', 'per', 'its',\n",
        "    'everything', 'behind', 'does', 'various', 'above', 'between', 'it', 'neither', 'seemed', 'ever', 'across', 'she',\n",
        "    'somehow', 'be', 'we', 'full', 'never', 'sixty', 'however', 'here', 'otherwise', 'were', 'whereupon', 'nowhere',\n",
        "    'although', 'found', 'alone', 're', 'along', 'quite', 'fifteen', 'by', 'both', 'about', 'last', 'would',\n",
        "    'anything', 'via', 'many', 'could', 'thence', 'put', 'against', 'keep', 'etc', 'amount', 'became', 'ltd', 'hence',\n",
        "    'onto', 'or', 'con', 'among', 'already', 'co', 'afterwards', 'formerly', 'within', 'seems', 'into', 'others',\n",
        "    'while', 'whatever', 'except', 'down', 'hers', 'everyone', 'done', 'least', 'another', 'whoever', 'moreover',\n",
        "    'couldnt', 'throughout', 'anyhow', 'yourself', 'three', 'from', 'her', 'few', 'together', 'top', 'there', 'due',\n",
        "    'been', 'next', 'anyone', 'eleven', 'cry', 'call', 'therefore', 'interest', 'then', 'thru', 'themselves',\n",
        "    'hundred', 'really', 'sincere', 'empty', 'more', 'himself', 'elsewhere', 'mostly', 'on', 'fire', 'am', 'becoming',\n",
        "    'hereby', 'amongst', 'else', 'part', 'everywhere', 'too', 'kg', 'herself', 'former', 'those', 'he', 'me', 'myself',\n",
        "    'made', 'twenty', 'these', 'was', 'bill', 'cant', 'us', 'until', 'besides', 'nevertheless', 'below', 'anywhere',\n",
        "    'nine', 'can', 'whether', 'of', 'your', 'toward', 'my', 'say', 'something', 'and', 'whereafter', 'whenever',\n",
        "    'give', 'almost', 'wherever', 'is', 'describe', 'beforehand', 'herein', 'doesn', 'an', 'as', 'itself', 'at',\n",
        "    'have', 'in', 'seem', 'whence', 'ie', 'any', 'fill', 'again', 'hasnt', 'inc', 'thereby', 'thin', 'no', 'perhaps',\n",
        "    'latter', 'meanwhile', 'when', 'detail', 'same', 'wherein', 'beside', 'also', 'that', 'other', 'take', 'which',\n",
        "    'becomes', 'you', 'if', 'nobody', 'unless', 'whereas', 'see', 'though', 'may', 'after', 'upon', 'most', 'hereupon',\n",
        "    'eight', 'but', 'serious', 'nothing', 'such', 'why', 'off', 'a', 'don', 'whereby', 'third', 'i', 'whole', 'noone',\n",
        "    'sometimes', 'well', 'amoungst', 'yours', 'their', 'rather', 'without', 'so', 'five', 'the', 'first', 'with',\n",
        "    'make', 'once','this'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysYEMVGExFgj"
      },
      "source": [
        "custom_stopwords=custom_stopwords.union(generic_stopwords)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9API8p3F4mol"
      },
      "source": [
        "df_sample=df.sample(withReplacement=False, fraction=0.02, seed=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epoSU3qkILwr",
        "outputId": "749b5a61-c5b5-43b7-a836-a3fae75ac5bb"
      },
      "source": [
        "df_sample.select(\"*\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+--------------------+--------------------+----------+\n",
            "|Polarity|               Title|              Review|len_Review|\n",
            "+--------+--------------------+--------------------+----------+\n",
            "|       2|Whispers of the W...|This was a easy t...|       270|\n",
            "|       2|Henry has come ho...|I had Henry back ...|       323|\n",
            "|       2| keeps his attention|My little boy is ...|       264|\n",
            "|       2|Grandson enjoyed ...|Fine. Boys and th...|       104|\n",
            "|       1|More than Thomas,...|I bought this thi...|       644|\n",
            "|       2|Simply excellent ...|I am a system ana...|       510|\n",
            "|       2|\"James Moody take...|\"James Moody, and...|       490|\n",
            "|       2|       Great Reading|Christine Feehan ...|       131|\n",
            "|       1|profanity the mov...|i cannot believe ...|       820|\n",
            "|       1|Pretty pathetic p...|\"The new \"\"stars ...|        99|\n",
            "|       2|   Simply brilliant!|\"This is a great ...|       334|\n",
            "|       1|            Horrible|This movie was ho...|       302|\n",
            "|       1|I'd rate it 0 sta...|\"Very few books c...|        68|\n",
            "|       2|     Great reference|The book was good...|       208|\n",
            "|       2|Why 5 stars? Beca...|A friend of mine ...|       389|\n",
            "|       2|   Wonderful classic|I enjoy this book...|        98|\n",
            "|       2|Good book if you ...|\"The Scarlet Lett...|       434|\n",
            "|       1|         complicated|I still haven't f...|       273|\n",
            "|       2|Adam Sandler is A...|This movie is gre...|       120|\n",
            "|       2|CAUGHT WITHOUT WO...|This album is a b...|       649|\n",
            "+--------+--------------------+--------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfF_AMcfP9-M"
      },
      "source": [
        "Helper functions for data cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkxTe9WA4o3C"
      },
      "source": [
        "from pyspark.sql.types import StringType\n",
        "from textblob import TextBlob, Word\n",
        "import re,string\n",
        "from gensim import utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2rNMuEsxqwi"
      },
      "source": [
        "def remove_stopwords(s):\n",
        "    s = utils.to_unicode(s)\n",
        "    return \" \".join(w for w in s.split() if w not in custom_stopwords)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_SMtRiR-xU4"
      },
      "source": [
        "def remove_puntuations(s):\n",
        "    RE_PUNCT = re.compile(r'([%s])+' % re.escape(string.punctuation), re.UNICODE)\n",
        "    s = utils.to_unicode(s)\n",
        "    return RE_PUNCT.sub(\" \", s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHBpIipi_jtS"
      },
      "source": [
        "def remove_tags(s):\n",
        "    RE_TAGS = re.compile(r\"<([^>]+)>\", re.UNICODE)\n",
        "    s = utils.to_unicode(s)\n",
        "    return RE_TAGS.sub(\" \", s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nh3audTz_4G7"
      },
      "source": [
        "def remove_multiplewhitespaces(s):\n",
        "    RE_WHITESPACE = re.compile(r\"(\\s)+\", re.UNICODE)\n",
        "    #s = utils.to_unicode(s)\n",
        "    return RE_WHITESPACE.sub(\" \", s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OSxaF6iB0KA"
      },
      "source": [
        "def to_lower(s):\n",
        "    s=utils.to_unicode(s)\n",
        "    return \" \".join(i.lower() for i in s.split())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXp9e9hxBVIB"
      },
      "source": [
        "def remove_smallwords(s,l=3):\n",
        "    s=utils.to_unicode(s)\n",
        "    return \" \".join(i for i in s.split() if len(i)>=l)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y052AFMuBGaY"
      },
      "source": [
        "def remove_nonalpha(s):\n",
        "    RE_NONALPHA = re.compile(r\"\\W\", re.UNICODE)\n",
        "    s = utils.to_unicode(s)\n",
        "    return RE_NONALPHA.sub(\" \", s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvkxtS1HA7nB"
      },
      "source": [
        "def remove_numerics(s):\n",
        "    RE_NUMERIC = re.compile(r\"[0-9]+\", re.UNICODE)\n",
        "    s = utils.to_unicode(s)\n",
        "    return RE_NUMERIC.sub(\" \", s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-l-Uc3ek0-9p"
      },
      "source": [
        "def lemmetizer(s):\n",
        "    s=TextBlob(s)\n",
        "    return \" \". join([w.lemmatize() for w in s.words])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8JxbENo7GaO"
      },
      "source": [
        "Pre_filters=[remove_tags,\n",
        "        remove_puntuations,\n",
        "        remove_multiplewhitespaces,\n",
        "        remove_numerics,\n",
        "        remove_smallwords,\n",
        "        to_lower,\n",
        "        remove_nonalpha,\n",
        "        lemmetizer,\n",
        "        remove_stopwords]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBRJSxWI7I-W"
      },
      "source": [
        "def pre_process(x):\n",
        "    review=x[2]\n",
        "    review=review.lower()\n",
        "    for f in Pre_filters:\n",
        "        review=f(review)\n",
        "    return(x[0],review)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zScO7XR-5MX1",
        "outputId": "35fba101-0e10-4fcc-b91d-4e9bc4e8c0cd"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "!python -m textblob.download_corpora"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "Finished.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlAs66jnlDxK"
      },
      "source": [
        "Clean Review Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Wb4IWsooMQw"
      },
      "source": [
        "def clean_text(df):\n",
        "    df=df.rdd.map(lambda x: pre_process(x))\n",
        "    df = df.toDF(['Polarity','Review'])\n",
        "    return(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2TtpJj87LLH"
      },
      "source": [
        "df_sample=clean_text(df_sample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WaSFG3PM7NXp",
        "outputId": "e220e7dd-d3b3-4b20-8ac0-6d2750539784"
      },
      "source": [
        "df_sample.select(\"*\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+--------------------+\n",
            "|Polarity|              Review|\n",
            "+--------+--------------------+\n",
            "|       2|read book want re...|\n",
            "|       1|bought charger in...|\n",
            "|       1|star depends look...|\n",
            "|       2|gift old daughter...|\n",
            "|       2|wow album track r...|\n",
            "|       2|completely satisf...|\n",
            "|       2|henry early origi...|\n",
            "|       2|henry dog lost ri...|\n",
            "|       2|little boy gettin...|\n",
            "|       2|whale naturalist ...|\n",
            "|       1|mask maker cinema...|\n",
            "|       1|formulation produ...|\n",
            "|       2|product hair curl...|\n",
            "|       1|buy game becuase ...|\n",
            "|       1|reviewing gizmo s...|\n",
            "|       2|fine boy toy trai...|\n",
            "|       1|bought thinking p...|\n",
            "|       1|search super save...|\n",
            "|       1|guerriula warfare...|\n",
            "|       2|analyst ibm backg...|\n",
            "+--------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0HrHFQSlXlY"
      },
      "source": [
        "Get most occuring words, to seee if any words is obvious to provide review polarity like good, bad etc and add them in custom stop words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RK8imq4y9a4",
        "outputId": "466ee105-131d-4a7d-afe2-7c6f252ff70e"
      },
      "source": [
        "df_sample.withColumn('word', F.explode(F.split(F.col('Review'), ' '))) \\\n",
        "  .groupBy('word') \\\n",
        "  .count() \\\n",
        "  .sort('count', ascending=False) \\\n",
        "  .take(200)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(word='book', count=104021),\n",
              " Row(word='time', count=40300),\n",
              " Row(word='movie', count=33085),\n",
              " Row(word='read', count=32982),\n",
              " Row(word='work', count=26661),\n",
              " Row(word='year', count=23087),\n",
              " Row(word='product', count=22652),\n",
              " Row(word='use', count=20336),\n",
              " Row(word='story', count=19954),\n",
              " Row(word='buy', count=19599),\n",
              " Row(word='bought', count=19586),\n",
              " Row(word='album', count=17948),\n",
              " Row(word='thing', count=17894),\n",
              " Row(word='way', count=17842),\n",
              " Row(word='new', count=16855),\n",
              " Row(word='song', count=16732),\n",
              " Row(word='know', count=16440),\n",
              " Row(word='think', count=16254),\n",
              " Row(word='little', count=16213),\n",
              " Row(word='doe', count=15805),\n",
              " Row(word='got', count=15588),\n",
              " Row(word='want', count=14878),\n",
              " Row(word='game', count=14689),\n",
              " Row(word='music', count=14563),\n",
              " Row(word='old', count=14184),\n",
              " Row(word='day', count=13987),\n",
              " Row(word='money', count=13825),\n",
              " Row(word='people', count=13794),\n",
              " Row(word='character', count=13729),\n",
              " Row(word='dvd', count=13078),\n",
              " Row(word='look', count=12756),\n",
              " Row(word='review', count=12676),\n",
              " Row(word='life', count=12246),\n",
              " Row(word='lot', count=12242),\n",
              " Row(word='sound', count=12229),\n",
              " Row(word='need', count=11859),\n",
              " Row(word='reading', count=11833),\n",
              " Row(word='quality', count=11496),\n",
              " Row(word='film', count=11249),\n",
              " Row(word='come', count=11194),\n",
              " Row(word='thought', count=11054),\n",
              " Row(word='fan', count=10578),\n",
              " Row(word='author', count=10341),\n",
              " Row(word='star', count=10030),\n",
              " Row(word='price', count=9766),\n",
              " Row(word='amazon', count=9697),\n",
              " Row(word='set', count=9641),\n",
              " Row(word='long', count=9503),\n",
              " Row(word='going', count=9334),\n",
              " Row(word='month', count=9262),\n",
              " Row(word='right', count=9082),\n",
              " Row(word='worth', count=8998),\n",
              " Row(word='looking', count=8973),\n",
              " Row(word='end', count=8852),\n",
              " Row(word='play', count=8703),\n",
              " Row(word='feel', count=8370),\n",
              " Row(word='purchased', count=8171),\n",
              " Row(word='different', count=8158),\n",
              " Row(word='series', count=8073),\n",
              " Row(word='real', count=7874),\n",
              " Row(word='watch', count=7834),\n",
              " Row(word='nice', count=7779),\n",
              " Row(word='written', count=7676),\n",
              " Row(word='page', count=7668),\n",
              " Row(word='version', count=7429),\n",
              " Row(word='child', count=7312),\n",
              " Row(word='far', count=7310),\n",
              " Row(word='item', count=7255),\n",
              " Row(word='bit', count=7183),\n",
              " Row(word='try', count=7132),\n",
              " Row(word='big', count=7109),\n",
              " Row(word='sure', count=6996),\n",
              " Row(word='waste', count=6927),\n",
              " Row(word='excellent', count=6840),\n",
              " Row(word='picture', count=6807),\n",
              " Row(word='actually', count=6770),\n",
              " Row(word='help', count=6723),\n",
              " Row(word='pretty', count=6710),\n",
              " Row(word='fun', count=6635),\n",
              " Row(word='tried', count=6563),\n",
              " Row(word='video', count=6496),\n",
              " Row(word='away', count=6490),\n",
              " Row(word='small', count=6458),\n",
              " Row(word='friend', count=6454),\n",
              " Row(word='second', count=6391),\n",
              " Row(word='came', count=6388),\n",
              " Row(word='world', count=6374),\n",
              " Row(word='week', count=6363),\n",
              " Row(word='interesting', count=6179),\n",
              " Row(word='fit', count=6095),\n",
              " Row(word='piece', count=6062),\n",
              " Row(word='band', count=6041),\n",
              " Row(word='original', count=6009),\n",
              " Row(word='said', count=5965),\n",
              " Row(word='kid', count=5873),\n",
              " Row(word='cover', count=5840),\n",
              " Row(word='battery', count=5796),\n",
              " Row(word='plot', count=5785),\n",
              " Row(word='getting', count=5781),\n",
              " Row(word='fact', count=5756),\n",
              " Row(word='minute', count=5720),\n",
              " Row(word='favorite', count=5706),\n",
              " Row(word='point', count=5697),\n",
              " Row(word='high', count=5661),\n",
              " Row(word='light', count=5632),\n",
              " Row(word='place', count=5627),\n",
              " Row(word='having', count=5601),\n",
              " Row(word='purchase', count=5583),\n",
              " Row(word='track', count=5538),\n",
              " Row(word='seen', count=5523),\n",
              " Row(word='novel', count=5506),\n",
              " Row(word='box', count=5475),\n",
              " Row(word='wanted', count=5456),\n",
              " Row(word='writing', count=5445),\n",
              " Row(word='line', count=5435),\n",
              " Row(word='idea', count=5417),\n",
              " Row(word='le', count=5413),\n",
              " Row(word='highly', count=5406),\n",
              " Row(word='received', count=5336),\n",
              " Row(word='start', count=5331),\n",
              " Row(word='enjoy', count=5330),\n",
              " Row(word='let', count=5328),\n",
              " Row(word='word', count=5297),\n",
              " Row(word='hour', count=5200),\n",
              " Row(word='instead', count=5191),\n",
              " Row(word='information', count=5182),\n",
              " Row(word='fine', count=5161),\n",
              " Row(word='tell', count=5125),\n",
              " Row(word='trying', count=5098),\n",
              " Row(word='able', count=5064),\n",
              " Row(word='heard', count=5060),\n",
              " Row(word='case', count=5045),\n",
              " Row(word='buying', count=5044),\n",
              " Row(word='ago', count=5043),\n",
              " Row(word='ordered', count=5031),\n",
              " Row(word='worked', count=4979),\n",
              " Row(word='job', count=4964),\n",
              " Row(word='believe', count=4941),\n",
              " Row(word='unit', count=4907),\n",
              " Row(word='won', count=4897),\n",
              " Row(word='took', count=4873),\n",
              " Row(word='order', count=4863),\n",
              " Row(word='family', count=4840),\n",
              " Row(word='isn', count=4831),\n",
              " Row(word='probably', count=4820),\n",
              " Row(word='maybe', count=4818),\n",
              " Row(word='guy', count=4797),\n",
              " Row(word='man', count=4790),\n",
              " Row(word='wish', count=4782),\n",
              " Row(word='went', count=4774),\n",
              " Row(word='reader', count=4766),\n",
              " Row(word='couldn', count=4758),\n",
              " Row(word='player', count=4736),\n",
              " Row(word='started', count=4704),\n",
              " Row(word='live', count=4692),\n",
              " Row(word='reason', count=4681),\n",
              " Row(word='home', count=4667),\n",
              " Row(word='style', count=4648),\n",
              " Row(word='son', count=4646),\n",
              " Row(word='enjoyed', count=4630),\n",
              " Row(word='hand', count=4625),\n",
              " Row(word='water', count=4605),\n",
              " Row(word='turn', count=4595),\n",
              " Row(word='size', count=4592),\n",
              " Row(word='hope', count=4564),\n",
              " Row(word='happy', count=4542),\n",
              " Row(word='especially', count=4539),\n",
              " Row(word='definitely', count=4516),\n",
              " Row(word='woman', count=4511),\n",
              " Row(word='true', count=4483),\n",
              " Row(word='understand', count=4469),\n",
              " Row(word='kind', count=4437),\n",
              " Row(word='gave', count=4408),\n",
              " Row(word='short', count=4397),\n",
              " Row(word='saw', count=4386),\n",
              " Row(word='liked', count=4361),\n",
              " Row(word='hold', count=4359),\n",
              " Row(word='listen', count=4348),\n",
              " Row(word='wasn', count=4334),\n",
              " Row(word='working', count=4242),\n",
              " Row(word='store', count=4217),\n",
              " Row(word='half', count=4216),\n",
              " Row(word='history', count=4205),\n",
              " Row(word='poor', count=4193),\n",
              " Row(word='daughter', count=4186),\n",
              " Row(word='funny', count=4171),\n",
              " Row(word='voice', count=4158),\n",
              " Row(word='scene', count=4154),\n",
              " Row(word='stuff', count=4099),\n",
              " Row(word='couple', count=4094),\n",
              " Row(word='left', count=4085),\n",
              " Row(word='making', count=4029),\n",
              " Row(word='girl', count=4006),\n",
              " Row(word='rock', count=3973),\n",
              " Row(word='collection', count=3970),\n",
              " Row(word='gift', count=3951),\n",
              " Row(word='experience', count=3939),\n",
              " Row(word='toy', count=3921),\n",
              " Row(word='amazing', count=3921),\n",
              " Row(word='phone', count=3903)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJ7hftJ0huAi"
      },
      "source": [
        "def Tokenize(df):\n",
        "    tokenizer = Tokenizer(inputCol=\"Review\", outputCol=\"Review_Words\")\n",
        "    wordsData = tokenizer.transform(df)\n",
        "    return(wordsData)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoZajgEt7RD4"
      },
      "source": [
        "def TFIDF(df):\n",
        "    wordsData=Tokenize(df)\n",
        "    hashingTF = HashingTF(inputCol=\"Review_Words\", outputCol=\"rawFeatures\", numFeatures=512)\n",
        "    featurizedData = hashingTF.transform(wordsData)\n",
        "\n",
        "    idf = IDF(inputCol=\"rawFeatures\", outputCol=\"Review_CV\")\n",
        "    IDFModel = idf.fit(featurizedData)\n",
        "    IDF_Data = IDFModel.transform(featurizedData)\n",
        "    return(IDF_Data)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRJU7WGLlbev"
      },
      "source": [
        "Get TFIDF features for review"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwdrXalBfHxF"
      },
      "source": [
        "IDF_Data=TFIDF(df_sample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNOk6Nd-lx0A"
      },
      "source": [
        "Define func for LogisticRegresion some grid search and CV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hLA2ien-Ae3"
      },
      "source": [
        "def LogictiveRegression_CV(df_train,df_val):\n",
        "    lr=LogisticRegression(labelCol='label',featuresCol='Review_CV')\n",
        "    pipeline = Pipeline(stages=[lr])\n",
        "    paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(lr.elasticNetParam, [0,0.2,0.7]) \\\n",
        "    .addGrid(lr.regParam, [0.1, 0.01,0.05]) \\\n",
        "    .build()\n",
        "    eval=BinaryClassificationEvaluator(labelCol='label')\n",
        "    crossval = CrossValidator(estimator=pipeline,\n",
        "                          estimatorParamMaps=paramGrid,\n",
        "                          evaluator=BinaryClassificationEvaluator(),numFolds=2) \n",
        "    cv=crossval.fit(df_train)\n",
        "    best_model=cv.bestModel.stages[0]\n",
        "    prediction=best_model.transform(df_val)\n",
        "    acc=eval.evaluate(prediction)\n",
        "    print('The accuracy is %g' %acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x92wQfn3l6WA"
      },
      "source": [
        "Perform test and Control split, also mlib expects the Output column to be names as 'label' with only 0/1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObWQVOJeNtvw"
      },
      "source": [
        "def train_test_Split(df):\n",
        "    df = df.withColumnRenamed('Polarity','label')\n",
        "    df = df.withColumn(\"label\", df[\"label\"].cast(IntegerType()))\n",
        "    df = df.withColumn(\"label\",when(df[\"label\"] == 2, 0).otherwise(df[\"label\"]))\n",
        "    df = df.withColumn(\"label\", df[\"label\"].cast(IntegerType()))\n",
        "    df_train,df_test=df.randomSplit([0.8,0.2])\n",
        "    return(df_train,df_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXwMR1dxHnyu"
      },
      "source": [
        "df_train,df_test=train_test_Split(IDF_Data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ssWO1S0nhPw"
      },
      "source": [
        "Repartition dataframe for faster processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbfc2IIieEhn"
      },
      "source": [
        "IDF_Data=IDF_Data.repartition(64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gjgx19Jmnm4L"
      },
      "source": [
        "Logistic Regression on TFIDF features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xg5-buNOEko3",
        "outputId": "668b27af-2548-4945-e4b8-e4ca66cd947a"
      },
      "source": [
        "LogictiveRegression_CV(df_train,df_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy is 0.74282\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNk1IDSgEnBT"
      },
      "source": [
        "def RandomForest_CV(df_train,df_val):\n",
        "    rf=RandomForestClassifier(labelCol='label',featuresCol='Review_CV')\n",
        "    pipeline = Pipeline(stages=[rf])\n",
        "    paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(rf.maxDepth, [10]) \\\n",
        "    .addGrid(rf.numTrees, [100]) \\\n",
        "    .addGrid(rf.impurity ,['gini','entropy']) \\\n",
        "    .addGrid(rf.maxBins,[20]) \\\n",
        "    .build()\n",
        "    eval=BinaryClassificationEvaluator(labelCol='label')\n",
        "    crossval = CrossValidator(estimator=pipeline,\n",
        "                          estimatorParamMaps=paramGrid,\n",
        "                          evaluator=BinaryClassificationEvaluator(),numFolds=2) \n",
        "    cv=crossval.fit(df_train)\n",
        "    best_model=cv.bestModel.stages[0]\n",
        "    prediction=best_model.transform(df_val)\n",
        "    acc=eval.evaluate(prediction)\n",
        "    print('The accuracy is %g' %acc)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KkXlRWi5Pdjy",
        "outputId": "7148615e-939b-45c4-eccc-83fac2151fbb"
      },
      "source": [
        "RandomForest_CV(df_train,df_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy is 0.809151\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yynUfbvmjpA"
      },
      "source": [
        "Although named as word2vec in mlib, Word2Vec created Doc2vec features of text by performing averaging on word2vec features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YjFbZL3bLQZ"
      },
      "source": [
        "def wor2vec(df):\n",
        "    wordsData=Tokenize(df)\n",
        "    w2v = Word2Vec(vectorSize=100, minCount=0, inputCol=\"Review_Words\", outputCol=\"Review_CV\")\n",
        "    w2v_model = w2v.fit(wordsData)\n",
        "    word2vec_TF = w2v_model.transform(wordsData)\n",
        "    return(word2vec_TF)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3Rze1uNYI-R"
      },
      "source": [
        "word2vec_TF=wor2vec(df_sample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viRmnBn68Qd0"
      },
      "source": [
        "word2vec_TF.select(\"*\").write.save('/content/word2vec_features.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2iEr6tN_oWZ"
      },
      "source": [
        "#word2vec_TF=spark.read.parquet('/content/word2vec_features.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxTN4sbVqFd6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlPq_1nr__iy",
        "outputId": "08481b8c-09be-43bb-a1f9-9fea04c90105"
      },
      "source": [
        "word2vec_TF.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+--------------------+--------------------+--------------------+\n",
            "|Polarity|              Review|        Review_Words|           Review_CV|\n",
            "+--------+--------------------+--------------------+--------------------+\n",
            "|       1|book look visuall...|[book, look, visu...|[0.00886286601710...|\n",
            "|       2|wanted write rewi...|[wanted, write, r...|[-0.0183582048843...|\n",
            "|       2|heard zucchero wa...|[heard, zucchero,...|[-0.0343871394354...|\n",
            "|       2|archer mayor book...|[archer, mayor, b...|[0.01174846675712...|\n",
            "|       2|enjoy series deal...|[enjoy, series, d...|[0.03125806953758...|\n",
            "|       1|gift set look big...|[gift, set, look,...|[0.01814348445197...|\n",
            "|       2|canon lens favori...|[canon, lens, fav...|[0.03245711780327...|\n",
            "|       2|book cute collage...|[book, cute, coll...|[-0.0104236687766...|\n",
            "|       1|blank dvd sale co...|[blank, dvd, sale...|[-0.0186463112272...|\n",
            "|       1|looking book fund...|[looking, book, f...|[-0.0114724895327...|\n",
            "|       1|buffy fan expecte...|[buffy, fan, expe...|[-0.0491798661674...|\n",
            "|       1|remember grade en...|[remember, grade,...|[0.03038680391220...|\n",
            "|       2|poor soul half wa...|[poor, soul, half...|[-0.0189317355183...|\n",
            "|       2|enjoyed reading b...|[enjoyed, reading...|[-0.0021659256225...|\n",
            "|       1|reviewer child ch...|[reviewer, child,...|[0.02542363896759...|\n",
            "|       1|thought baby mesm...|[thought, baby, m...|[0.00264350540237...|\n",
            "|       2|baby stay chair h...|[baby, stay, chai...|[-0.0180595239455...|\n",
            "|       2|book incredible b...|[book, incredible...|[0.03211400838169...|\n",
            "|       1|began reading boo...|[began, reading, ...|[0.01657003855022...|\n",
            "|       2|tozer favorite bo...|[tozer, favorite,...|[-0.0072285465139...|\n",
            "+--------+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HdOaZOt6K7w"
      },
      "source": [
        "word2vec_TF = word2vec_TF.orderBy(rand())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBpa7gTdncpB"
      },
      "source": [
        "word2vec_TF=word2vec_TF.withColumnRenamed('w2v_Features','Review_CV')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDkXARS5webR"
      },
      "source": [
        "df_train,df_test=train_test_Split(word2vec_TF)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMgd2g-AmPcz"
      },
      "source": [
        "Perform Logistic Regression on Doc2Vec features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epcPqpWonIjf",
        "outputId": "ed79d876-4944-483a-8e4b-90c9a0e2db7f"
      },
      "source": [
        "LogictiveRegression_CV(df_train,df_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy is 0.827134\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXNm51HkoiUQ"
      },
      "source": [
        "Train Word2vec on bigger sample of text data and build classification model for previously sampled data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3aDvtDXpqSn"
      },
      "source": [
        "def wor2vec(df):\n",
        "    wordsData=Tokenize(df)\n",
        "    w2v = Word2Vec(vectorSize=100, minCount=2, inputCol=\"Review_Words\", outputCol=\"Review_CV\",numPartitions=64)\n",
        "    w2v_model = w2v.fit(wordsData)\n",
        "    return(w2v_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGuCbuMXrCa8"
      },
      "source": [
        "df=df.repartition(12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjZgYVf0O8x3"
      },
      "source": [
        "Take 30-50% sample from original corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqfPP6RFxPL9"
      },
      "source": [
        "df_sample2=df.sample(False,fraction=.3,seed=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oor0DFHv9kc1",
        "outputId": "14891dc1-1c2b-4022-ae9c-098ed9d12d29"
      },
      "source": [
        "df_sample2.select('*').count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1079838"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhXRWvH6ol69"
      },
      "source": [
        "df_sample2=clean_text(df_sample2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSCat-hqpWsI"
      },
      "source": [
        "word2vec_full=wor2vec(df_sample2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsnduG5bqZat"
      },
      "source": [
        "word2vec_full.save('/content/word2vec_model_50')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ti62ivk8elS"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/word2vec_model_30')\n",
        "# copy it there\n",
        "!cp model content/word2vec_model_30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBHS4ZEdXfbo"
      },
      "source": [
        "df_sample is previously sampled ~70k corpus of data with slmost similar distribution for both classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsI6JEikBKRt"
      },
      "source": [
        "df_sample=Tokenize(df_sample)\n",
        "df_sample=word2vec_full.transform(df_sample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nnzhHq1GDst"
      },
      "source": [
        "df_train,df_test=train_test_Split(df_sample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3ytkFCuJlw9"
      },
      "source": [
        "word2vec trained on bigger sample(~1M reviews), gave 4% boost in accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0D7qw2FZGUFT",
        "outputId": "22cdf502-5b40-429c-fdbf-13f6d9278aa5"
      },
      "source": [
        "LogictiveRegression_CV(df_train,df_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy is 0.86459\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvFkzBd2wqjn"
      },
      "source": [
        "word2vec trained on bigger sample(~1.5M reviews), gave 5% boost in accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvQqiPGmtPhO",
        "outputId": "eef960f9-f8b1-4dbf-86b7-253d21dd61df"
      },
      "source": [
        "LogictiveRegression_CV(df_train,df_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy is 0.875109\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzFo9jeDZr3k"
      },
      "source": [
        "Create a deep learning model with keras, utilize Elephas to use distributed computing of spark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48HCBAW8dQ6m"
      },
      "source": [
        "df_train=df_train.filter(df_train['Review']!=\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngL3aiLdeYQ4",
        "outputId": "a2b75e56-e2fe-4d4c-da0f-33d1ebd71779"
      },
      "source": [
        "df_train.groupby('label').count().show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+-----+\n",
            "|label|count|\n",
            "+-----+-----+\n",
            "|    1|28756|\n",
            "|    0|28824|\n",
            "+-----+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8DGBm79gxvz"
      },
      "source": [
        "#print(len(df_train.select(\"Review_CV\").first()[0]))\n",
        "input_dim=100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlitPlbtdG_k"
      },
      "source": [
        "def build_classifier_model():\n",
        "    model = tf.keras.Sequential([\n",
        "      layers.Dense(128,activation='relu',input_shape=(input_dim,),activity_regularizer=regularizers.l2(0.02)),\n",
        "      layers.Dropout(.5),\n",
        "      layers.BatchNormalization(),\n",
        "      layers.Dense(64,activation='relu'),\n",
        "      layers.Dropout(.4),\n",
        "      layers.BatchNormalization(),\n",
        "      layers.Dense(16,activation='relu'),\n",
        "      layers.Dropout(.4),\n",
        "      layers.Dense((2),activation='softmax')])\n",
        "    return(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQl1hp4WhANG"
      },
      "source": [
        "classifier_model=build_classifier_model()\n",
        "classifier_model.compile(optimizer='Adam',loss='binary_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqfb3FRClyqO",
        "outputId": "aea8c7b4-09f9-496b-b2ba-12037ce9ca93"
      },
      "source": [
        "\n",
        "optimizer_conf = optimizers.Adam()\n",
        "opt_conf = optimizers.serialize(optimizer_conf)\n",
        "\n",
        "# Initialize SparkML Estimator\n",
        "estimator = ElephasEstimator()\n",
        "estimator.setFeaturesCol(\"Review_CV\")\n",
        "estimator.setLabelCol(\"label\")\n",
        "estimator.set_keras_model_config(classifier_model.to_json())\n",
        "estimator.set_categorical_labels(True)\n",
        "estimator.set_nb_classes(2)\n",
        "estimator.set_num_workers(16)\n",
        "estimator.set_epochs(20) \n",
        "estimator.set_batch_size(128)\n",
        "estimator.set_verbosity(4)\n",
        "estimator.set_validation_split(0.20)\n",
        "estimator.set_optimizer_config(opt_conf)\n",
        "estimator.set_mode(\"synchronous\")\n",
        "estimator.set_loss(\"binary_crossentropy\")\n",
        "estimator.set_metrics(['acc'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ElephasEstimator_e3f24cf6708d"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffJVQr8ewRbA",
        "outputId": "1add4de2-d58b-4d37-8809-b597e30c437f"
      },
      "source": [
        "dl_pipeline = Pipeline(stages=[estimator])\n",
        "fit_dl_pipeline = dl_pipeline.fit(df_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">>> Fit model\n",
            ">>> Synchronous training complete.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFZ6TzoJv1VL"
      },
      "source": [
        " pred = fit_dl_pipeline.transform(df_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iE41HBffxeCS"
      },
      "source": [
        "pred_filter=pred.select(\"label\", \"prediction\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ieRnEh1kFP0"
      },
      "source": [
        "pred_filter=pred_filter.repartition(64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cl0L6mahCrsh"
      },
      "source": [
        "test_label_pred = pred_filter.toPandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcoWm6Ih1G5z"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJeJX3O0zSi8"
      },
      "source": [
        "test_label_pred['prediction']=test_label_pred['prediction'].apply(lambda x: np.argmax(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xiy1U9KQBtcw"
      },
      "source": [
        "#test_label_pred[\"prediction\"]=test_label_pred[\"prediction\"].apply(lambda x:1 if x>0.5 else 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4Cty_thBNzd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f337c0b-b95c-496b-ce0c-3519f887b667"
      },
      "source": [
        "accuracy_score(test_label_pred['label'],test_label_pred['prediction'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7317876155785934"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyrvFS15cJCv"
      },
      "source": [
        "# Try SparkNLP based sentence bert model, and building model based on DNN based spark model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NV7xUAXkcMzv"
      },
      "source": [
        "!pip install sparknlp\n",
        "!pip install pyspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QamfisxZc7MN"
      },
      "source": [
        "import sparknlp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJ0dQd0TnoAS"
      },
      "source": [
        "Word of caution, if sparksession is enabled then documentassembler here will give error"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-gx3qoAdHAf"
      },
      "source": [
        "spark = sparknlp.start() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsk-rch8dcIe"
      },
      "source": [
        "from sparknlp.base import *\n",
        "from sparknlp.annotator import *\n",
        "from pyspark.ml import Pipeline\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYMEQ_kJd9ZB"
      },
      "source": [
        "document = DocumentAssembler()\\\n",
        "    .setInputCol(\"Review\")\\\n",
        "    .setOutputCol(\"Review_ann\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIqGayDsFySA"
      },
      "source": [
        "Try Bert Model(small uncased)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJ25LErBeDQV",
        "outputId": "172f4a76-9799-4fc2-8fb6-6c0f2eae7f41"
      },
      "source": [
        "Bert_pretrained = BertSentenceEmbeddings.pretrained('sent_small_bert_L2_768')\\\n",
        " .setInputCols([\"Review_ann\"])\\\n",
        " .setOutputCol(\"sentence_embeddings\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sent_small_bert_L2_768 download started this may take some time.\n",
            "Approximate size to download 139.6 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIDMMvZ1eQw8"
      },
      "source": [
        "classsifierdl = ClassifierDLApproach()\\\n",
        "  .setInputCols([\"sentence_embeddings\"])\\\n",
        "  .setOutputCol(\"class\")\\\n",
        "  .setLabelColumn(\"label\")\\\n",
        "  .setMaxEpochs(5)\\\n",
        "bert_clf_pipeline = Pipeline(\n",
        "    stages = [\n",
        "        document,\n",
        "        Bert_pretrained,\n",
        "        classsifierdl\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNcw_4UejEV5"
      },
      "source": [
        "Pre_filters=[remove_tags,\n",
        "        remove_puntuations,\n",
        "        remove_multiplewhitespaces,\n",
        "        remove_numerics,\n",
        "        #remove_smallwords,\n",
        "        #to_lower,\n",
        "        remove_nonalpha\n",
        "        #lemmetizer,\n",
        "        #remove_stopwords\n",
        "        ]\n",
        "         ## remove some preprocessing for bert based model cleaning "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Us6AvKc4nRV6"
      },
      "source": [
        "df_sample=clean_text(df_sample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpK26MFymlWB"
      },
      "source": [
        "from pyspark.sql.functions import trim\n",
        "df_sample=df_sample.withColumn(\"Review\",trim(df_sample.Review))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQ8WQDO3SaA5"
      },
      "source": [
        "df_train,df_test=train_test_Split(df_sample) # text cleaning done based on previous model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YB_tv-uSfzse"
      },
      "source": [
        "bert_pipelineModel = bert_clf_pipeline.fit(df_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auZFu40zhCx4"
      },
      "source": [
        "use_pipelineModel.save('/content/bertsentence/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOxrp889RWUe"
      },
      "source": [
        "df_test=use_pipelineModel.transform(df_test).select('Review','label','class.result')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYbXiDadh3qC"
      },
      "source": [
        "df_test=df_test.toPandas()\n",
        "df_test['result']=df_test['result'].apply(lambda x: x[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7xlafg-mhEK"
      },
      "source": [
        "df_test.result=pd.to_numeric(df_test.result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEvNMdMHiMuk",
        "outputId": "d3c5d00a-ba89-416b-c959-d5f0bdf629ae"
      },
      "source": [
        "print(accuracy_score(df_test.label,df_test.result))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8137492219378933\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3NHxUmKFuIw"
      },
      "source": [
        "Try Universal Sentence Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hl0m6ikFBxPb",
        "outputId": "edb8d04d-26cd-4e3b-9508-a72c68481113"
      },
      "source": [
        "USE_model = UniversalSentenceEncoder.pretrained()\\\n",
        " .setInputCols([\"Review_ann\"])\\\n",
        " .setOutputCol(\"sentence_embeddings\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tfhub_use download started this may take some time.\n",
            "Approximate size to download 923.7 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQWWEl0BFeQt"
      },
      "source": [
        "classsifierdl = ClassifierDLApproach()\\\n",
        "  .setInputCols([\"sentence_embeddings\"])\\\n",
        "  .setOutputCol(\"class\")\\\n",
        "  .setLabelColumn(\"label\")\\\n",
        "  .setMaxEpochs(5)\n",
        "use_clf_pipeline = Pipeline(\n",
        "    stages = [\n",
        "        document,\n",
        "        USE_model,\n",
        "        classsifierdl\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnOHEsUTGHQr"
      },
      "source": [
        "use_pipelineModel = use_clf_pipeline.fit(df_train) ## bert model training took an hour, USE took 4 min"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4JM19RXGQJC"
      },
      "source": [
        "df_test=use_pipelineModel.transform(df_test).select('Review','label','class.result')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhNKze33HVXC"
      },
      "source": [
        "df_test=df_test.toPandas()\n",
        "df_test['result']=df_test['result'].apply(lambda x: x[0])\n",
        "df_test.result=pd.to_numeric(df_test.result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LS134EeZHfYf",
        "outputId": "5e198697-bf3e-4bbf-8b1e-d5d172087a78"
      },
      "source": [
        "print(accuracy_score(df_test.label,df_test.result))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8401590734668248\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_xLXnmcPEUQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}